# 机器学习绪论核心概念总结

## 1.1 引言
- **机器学习本质**：通过算法从数据中学习规律（模型）
- **算法 vs 模型**：
  - 算法：学习方法（如线性回归、决策树）
  - 模型：算法产出的函数（如f(x)=wx+b）
  - 实际使用时二者常混用

## 1.2 基本术语
| 术语 | 别名 | 说明 |
|------|------|------|
| **样本** | 示例 | 事件/对象的特征向量表示（如x=(青绿;蜷缩;清脆)） |
| **特征** | 属性 | 描述样本的维度（如色泽、根蒂） |
| **数据集** | - | 样本的集合D={x₁,x₂,...,xₘ} |
| **标记** | 输出 | 样本的预测目标值y（如好瓜=1） |
| **样本空间** | 输入空间/属性空间 | 所有可能样本构成的空间𝒳 |
| **标记空间** | 输出空间 | 所有可能标记构成的空间𝒴 |

### 任务分类
1. **按标记类型**：
   - **分类**（离散值）：二分类（Y={0,1}）、多分类
   - **回归**（连续值）：Y=ℝ

2. **按标记使用**：
   - **监督学习**：训练使用标记（如线性回归）
   - **无监督学习**：训练不使用标记（如聚类）

### 关键概念
- **泛化能力**：模型对未知数据的预测能力
- **独立同分布假设**：样本从同一分布𝒟独立采样获得
- **数据决定上限**：数据质量和特征工程决定模型潜力
- **算法逼近上限**：算法决定模型能达到的实际效果

## 1.3 假设空间
- **假设空间**：所有可能模型的集合（如所有线性函数）
- **版本空间**：与训练集一致的假设子集
- **示例**：
  - 房价预测问题中：
    - 假设空间1：一元一次函数（y=3x-2）
    - 假设空间2：一元二次函数（y=x²）

## 1.4 归纳偏好
- **定义**：算法对某种类型假设的偏好
- **评估方法**：
  1. 奥卡姆剃刀原则（选择简单假设）
     **核心思想**：在同样能解释观测现象的假设中，选择最简单的那个  
      **何为"简单"**：  
      - 参数更少（如线性模型 vs 高阶多项式）
      - 结构更简洁（如决策树深度更浅）
      - 计算复杂度更低  
      
      **实例解析**：  
      ```python
      # 解释相同数据集的两种模型
      数据集 = [(1,3), (2,5), (3,7)]
      
      # 复杂模型：三次多项式
      f_complex(x) = 0.5x³ - 2x² + 6.5x - 2
      
      # 简单模型：线性函数
      f_simple(x) = 2x + 1
      
      # 两个模型在训练集上都完美拟合：
      # f_complex(1)=3, f_complex(2)=5, f_complex(3)=7
      # f_simple(1)=3, f_simple(2)=5, f_simple(3)=7
      
      ```
      选择决策：
      根据奥卡姆剃刀原则，优先选择f_simple，因为：
      - 参数更少（2个 vs 4个）
      - 结构更简洁（线性 vs 立方）
      - 更可能反映真实规律而非噪声拟合

  2. 测试集表现（更常用）
- **核心结论**：没有绝对最优算法，只有适合具体问题的算法

## Tips
**"数据决定上限，算法逼近上限"**
